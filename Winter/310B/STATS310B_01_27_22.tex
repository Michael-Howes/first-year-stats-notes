\include{preamble}
\include{definitions}



\title{STATS310B -- Lecture 8}
\author{Sourav Chatterjee\\ Scribed by Michael Howes}
\date{01/27/22}

\pagestyle{fancy}
\fancyhf{}
\rhead{STATS310B -- Lecture 8}
\lhead{01/27/22}
\rfoot{Page \thepage}

\begin{document}
\maketitle
\tableofcontents
\section{Polya's Urn}
Consider an urn of infinite capacity. The urn initially has one white ball and one black ball inside it. At each time step, a ball is picked uniformly at random from the urn and replaced back into the urn with another ball of the same color. Equivalently we choose a color with probability proportional to the number of balls of the same color and then but in an additional ball of the chosen color. 

Let $W_n$ be the proportion of white ball at time $n$ with $W_0=\frac{1}{2}$. We would like to understand the limiting behavior of $W_n$ as $n \to \infty$.
\begin{proposition}
    Let $\F_n = \sigma(W_1,\ldots, W_n)$. Then the sequence $\{W_n\}_{n \ge 0}$ is a martingale with respect to $\{\F_n\}_{n\ge 0}$.
\end{proposition}
\begin{proof}
    Note that the total number of balls at time $n$ is $n+2$. Let $N_n$ be the number of white ball in the urn at time $n$. Thus, $W_n = \frac{1}{n+2}N_n$. It follows that,
    \begin{align*}
        \E(W_{n+1}|\F_n) &= \frac{1}{n+3}\E(N_{n+1}|\F_n)\\
        &=\frac{1}{n+3}\left((N_{n}+1)\frac{N_n}{n+2}+N_n\frac{n+2-N_n}{n+2}\right)\\
        &=\frac{1}{n+3}\left(\frac{1}{n+2}N_n^2 + \frac{N_n}{n+2}+N_n-\frac{1}{n+2}N_n^2\right)\\
        &=\frac{1}{n+3}\left(\frac{n+3}{n+2}N_n\right)\\
        &=\frac{1}{n+2}N_n\\
        &=W_n.\qedhere 
    \end{align*}
\end{proof}
Note that $W_n \in [0,1]$ for every $n$ and thus $\sup_n \E[W_n^+]\le 1< \infty$. It follows that there exists an integrable random variable $W$ such that $W_n \to W$ almost surely. We will in fact prove that $W$ is uniformly distributed on $[0,1]$.
\begin{proof}
    We will show by induction that for all $n$, $N_n$ is uniformly distributed on $\{1,2,\ldots,n+1\}$, where $N_n$ is the number of white balls at time $n$. This is true when $n=0$ since $N_0 = 1$. Now suppose that the result is true for some $n$. Then, for $k=1,\ldots,n+2$,
    \begin{align*}
        \Pa(M_{n+1} = k) &=\sum_{j=1}^{n+1} \Pa(M_{n+1}=k|N_n = j)\Pa(N_n=j)\\
        &=\frac{1}{n+1}\sum_{j=1}^{n+1} \Pa(M_{n+1}=k|N_n = j).
    \end{align*}
    Note that $\Pa(M_{n+1}=k|N_n=j) \neq 0$  only if $j = k-1$ or $j=k$. This is even true if $k=1$ or $k=n+2$ although in these cases one  $\Pa(M_{n+1}=k|N_n=k-1) =0$ or $\Pa(M_{n+1}=k|N_n=k)=0$ respectively which agrees with the calculations below. Thus,
    \begin{align*}
        \Pa(M_{n+1} = k) &=\frac{1}{n+1}\left(\Pa(M_{n+1}=k|N_n=k-1)+\Pa(M_{n+1}=k|N_n=k)\right)\\
        &=\frac{1}{n+1}\left(\frac{k-1}{n+2}+\frac{n+2-k}{n+2}\right)\\
        &=\frac{1}{n+1}\left(\frac{n+1}{n+2}\right)\\
        &=\frac{1}{n+2}.
    \end{align*}
    Thus, $N_{n+1}$ is uniformly distributed on $\{1,\ldots,n+2\}$. Hence, $W_n$ is uniformly distributed on $\left\{\frac{1}{n+2},\ldots,\frac{n+1}{n+2}\right\}$ which implies $W_n$ converges in distribution to $U[0,1]$ but $W_n$ also converges almost surely (and this $W_n \to W$ in distribution). Thus, $W$ must be uniformly distributed on $[0,1]$.
\end{proof}
\section{L\'evy's downwards convergence theorem}
Our next convergence theorem is L\'evy's downwards convergence theorem which is also called the backwards martingale theorem. 
\begin{theorem}[L\'evy's downards convergence theorem]
    Let $X$ be an integrable random variable defined on $(\Om,\F,\Pa)$. Let $\F_0 \supseteq \F_1 \supseteq \F_2 \supseteq \ldots$ be a decreasing sequence of sub-$\sigma$-algebras of $\F$. Define $\F^* = \bigcap_{n=0}^\infty \F_n$, then 
    \[\E(X|\F_n) \to \E(X|\F^*), \]
    almost surely and in $L^1$.
\end{theorem}
\begin{proof}
    Let $X_n = \E(X|\F_n)$. We will first show that $\{X_n\}_{n \ge 0}$ has an almost sure limit $X^*$. We will then prove that $X_n$ converges to $X^*$ in $L^1$ and then finally we will show that $X^*=\E(X|\F^*)$. Fix $n \in \N$ and consider the time reversed finite sequence,
    \[X_{n}, X_{n-1},X_{n-2},\ldots,X_0. \]
    The above sequence is a martingale with respect to $\F_n,\F_{n-1},\ldots,\F_0$. This is because $\F_{k} \subseteq \F_{k-1}$ and thus
    \[\E(X_{k-1}|\F_{k})  = \E(\E(X|\F_{k-1})|\F_k)=\E(X|\F_k). \]
    Fix an interval $[a,b]$ and let $U_n$ be the number of complete upcrossings of $[a,b]$ by $X_{n},X_{n-1},\ldots,X_0$. By the upcrossing lemma,
    \begin{align*}
        \E[U_n] &\le \frac{\E[(X_0-a)^+]-\E[(X_n-a)^+]}{b-a}\\
        &\le \frac{\E[(X_0-a)^+]}{b-a}\\
        &= \frac{\E[(\E(X|\F_0)-a)^+]}{b-a}\\
        &\le \frac{\E[\E((X-a)^+|\F_0)]}{b-a}\\
        &\le \frac{\E[(X-a)^+]}{b-a}.
    \end{align*}
    Note that $U_n \le U_{n+1}$ and thus $U_n \nearrow U$ for some random variable $U$. By the monotone convergence theorem, $\E[U] \le \frac{\E[(X-a)^+]}{b-a}<\infty$. As with Doob's sub-martingale convergence theorem, this implies that $X^* = \lim_{n \to \infty} X_n$ exists almost surely. We will now show that $X^*$ is integrable. Note that
    \begin{align*}
        \E[\abs{X^*}] &= \E\left[\lim_{n \to \infty}\abs{X_n}\right]\\
        &\le \liminf_{n \to \infty}\E[\abs{X_n}]\\
        &=\liminf_{n\to \infty} \E[\abs{\E(X|\F_n)}]\\
        &\le \liminf_{n \to \infty} \E[\E(\abs{X}|\F_n)]\\
        &=\liminf_{n \to \infty} \E[\abs{X}]\\
        &=\E[\abs{X}]<\infty.
    \end{align*} 
    To show that $X^* =\E(X|\F^*)$ and that $X_n \to X^*$ in $L^1$ we need to first review the concept of \emph{uniform integrability}.
\end{proof}

\section{Uniform integrability}
\begin{definition}
    A sequence of random variables $\{X_n\}_{n \ge 1}$ is \emph{uniformly integrable} if for $\epsilon > 0$, there exists $K > 0$ such that,      
    \[\sup_n \E[\abs{X_n}\one_{\{\abs{X_n}>k\}}] < \eps. \]
\end{definition}
Uniform integrability allows one to calculate the expectation of a limit.
\begin{lemma}
    Suppose that $\{X_n\}_{n \ge 0}$ is a uniformly integrable sequence and $X_n \to X$ almost surely. Then $X$ is integrable and $X_n \to X$ in $L^1$.
\end{lemma}
\begin{proof}
    Take any $\eps > 0$ and take $k$ such that $\E[\abs{X_n}\one_{\{\abs{X_n}>k\}}]<\eps$. Then, 
    \[\E[\abs{X_n}] \le \E[\abs{X_n}\one_{\{X_n >k\}}] +\E[\abs{X_n}\one_{\{X_n\le k\}}]\le \eps+k. \]
    Thus, $\E[\abs{X}] \le \liminf_n \E[\abs{X_n}] \le \eps+k<\infty$. So $X$ is integrable. Note that for all $L > 0$, $\abs{X}\one_{\{\abs{X} > L\}} \le \abs{X}$ and, almost surely 
    \[ \lim_{L \to \infty} \abs{X}\one_{\{\abs{X}> L\}} = 0.\]
    Thus, by the dominated convergence theorem,
    \[\lim_{L \to \infty} \E[\abs{X}\one_{\{\abs{X}>L\}}] = 0. \]
    This shows that given $\eps > 0$, we can choose $k>0$ so that $\E[\abs{X}\one_{\{\abs{X}>k\}}]<\eps$ and
    \[\sup_n \E[\abs{X_n}\one_{\{\abs{X_n}>k\}}] <\eps.\] 
    Let $\eps > 0$ be arbitrary and fix such a corresponding $k>0$. Let $\phi : \R \to \R$ be given by
    \[\phi(x) = \begin{cases}
        -k & \text{if } x \le -k,\\
        x & \text{if } x \in (-k,k),\\
        k & \text{if } x \ge k.
    \end{cases} \]  
    The function $\phi$ is bounded and continuous and $\abs{\phi(x)-x}\le \abs{x}\one_{\{\abs{x}>k\}}$ for all $x \in \R$. Thus,
    \begin{align*}
        \E[\abs{X_n-X}]&\le \E[\abs{X_n-\phi(X_n)}] + \E[\abs{\phi(X_n)-\phi(X)}] +\E[\abs{\phi(X)-X}]\\
        &\le \E[\abs{X_n}\one_{\{\abs{X_n}> k\}}] + \E[\abs{\phi(X_n)-\phi(X)}]+\E[\abs{X}\one_{\{\abs{X} > k\}}]\\
        &< 2\eps + \E[\abs{\phi(X_n)-\phi(X)}].
    \end{align*}
    The random variable $\abs{\phi(X_n)-\phi(X)}$ is bounded above by $2k$ and goes to 0 almost surely. Thus, by the dominated convergence theorem,
    \begin{align*}
        \limsup_n \E[\abs{X_n-X}] &\le 2\eps + \limsup_n \E[\abs{\phi(X_n)-\phi(X)}] = 2\eps.
    \end{align*}
    Thus, $X_n \to X$ in $L^1$.
\end{proof}
We will next state a characterization of uniform integrability that we will need in proving L\'evy's downwards convergence theorem.
\begin{proposition}
    Let $\{X_n\}_{n \ge 1}$ be a sequence of random variables. The sequence $\{X_n\}_{n \ge 0}$ is uniformly integrable if and only if the following both hold,
    \begin{enumerate}
        \item $\sup_n \E[\abs{X_n}] < \infty$.
        \item For all $\eps >0$ there exists $\delta > 0$ such that for all $A$ and $n$, if $\Pa(A) <\delta$, then $\E[\abs{X_n}\one_A] < \eps$.
    \end{enumerate}
\end{proposition}
We will prove this proposition in the next lecture.
\end{document}