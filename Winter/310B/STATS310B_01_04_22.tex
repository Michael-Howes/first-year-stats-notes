\include{preamble}
\include{definitions}



\title{STATS310B -- Lecture 1}
\author{Sourav Chatterjee\\ Scribed by Michael Howes}
\date{01/04/22}

\pagestyle{fancy}
\fancyhf{}
\rhead{STATS310B -- Lecture 1}
\lhead{01/04/22}
\rfoot{Page \thepage}

\begin{document}
\maketitle
\tableofcontents
\section{Announcements}
An email will be sent out with administrative information. The course will cover two main topics: conditional expectation and martingales. There will also be some applications to Markov chains and ergodic theory.
\section{Conditional expectation}
\subsection{Motivation}
The quantities $P(A|X=x)$ and $E[Y|X=x]$ are clear when $X$ is a discrete random variable. In this case we have 
\[P(A |X=x) = \frac{P(A \cap \{X=x\})}{P(X=x)}, \]
when $P(X=x) > 0$. It is unclear how we can define $P(A|X=x)$ when $P(X=x)=0$, but this is important when $X$ is a continuous random variable. We can normally interpret $P(A|X=x)$ as
\[\lim_{\eps \to 0} P(A |X\in (x-\eps,x+\eps)). \]
But this immediately leads to problems. Notably, when does the limit exist? Do we have to put restrictions on $A$ or $X$?, etc.  There is also a big problem called the Borel--Kolmogorov paradox. One version of the paradox is as follows (see \href{https://en.wikipedia.org/wiki/Borel%E2%80%93Kolmogorov_paradox}{wikipedia} for another example). 
\begin{example}
    Let $(X,Y)$ be uniformly distributed on the unit disk in $\R^2$. What is the distribution of $Y$ given $X=0$? That is what is the distribution of $Y$, given that $(X,Y)$ is on the $y$-axis? The answer is that $Y$ should be uniform on $[-1,1]$. Indeed, if we take limits of the conditional distribution given $X\in (-\eps,\eps)$, then we do get the uniform distribution. 

    But the event $\{X=0\}$ can be written in a different way. Let $(R,\Ta)$ be $(X,Y)$ in polar coordinates with $\Ta \in [0,2\pi)$.  Then $\{X=0\}=\{\Ta = \pi/2\} \cup \{\Ta = 3\pi/2\}$. So we can think of the conditional distribution of $Y$ given $X=0$ as the limit of the conditional distribution of $Y$ given $\Ta \in (\pi/2-\eps,\pi/2+\eps) \cup(3\pi/2 - \eps, 3\pi/2+\eps)$. But this limiting distribution is not uniform on $[-1,1]$. The limiting distribution is proportional to $\abs{y}$. This is because we are now approximating $\{X=0\}$ as two skinny wedges rather than a vertical strip.
\end{example} 
The upshot of this example is as follows. Conditioning on events of probability zero is not well-defined as the limit of conditioning on approximating events of positive probability. The limit may depend on the choice of approximating events. 

Here is one resolution to the paradox. We cannot view the conditional distribution of $Y$ given $X=0$ as the same object as the conditional distribution of $Y$ given $\Ta \in \{\pi/2, 3\pi/2\}$. What remains true is that we can still compute $P(Y \in A |X=x)$ by using a limiting procedure. Fixing $A$ we can define a function $f(x) = P(Y \in A |X=x)$. We can interpret $P(Y \in A|X=0)$ as $f(0)$. We can also define a function $g(\ta) = P(Y \in A|\Ta=\ta)$, but these are different functions. Then we can plug $X$ or $\Ta$ into $f$ and $g$ to get random variables. The random variable $f(X)$ is what's written as $P(Y \in A|X)$. Thus, conditional probabilities are random variables.

In rigorous conditional probability, we define conditional expectations of a random variable $Y$ given a $\sigma$-algebra $\G$ instead of an event. The conditional expectation is a random variable and is written as $E[Y|\G]$. In our examples $\G$ was $\sigma(X)$.

\subsection{Definition, uniqueness and existence}
We will now give the rigorous definition of conditional expectation.
\begin{definition}
    Let $(\Om,\F,P)$ be a probability space. Let $X$ be a real valued integrable random variable defined on $(\Om,\F)$ and let $\G \subseteq \F$ be a sub-$\sigma$-algebra of $\F$. An integrable random variable $Y$ is called $E[X|\G]$ if
    \begin{enumerate}
        \item[(1)] The random variable $Y$ is $\G$-measurable.
        \item[(2)] For all $B \in \G$, $E[Y\one_B]=E[X\one_B ]$, where $\one_B$ denotes the indicator function of $B$.  
    \end{enumerate}
\end{definition}
\begin{lemma}\label{uniqueness-lemma}
    If $E[X|\G]$ exists, then it is unique almost surely.
\end{lemma}
Before we prove the above lemma, let's consider an example.
\begin{example}
    Again consider $(X,Y)$ uniformly distributed on the unit disk. Let $\G = \sigma(X)$, what is $E[Y^2|\G]$? (Note: usually we will write $E[Y|X]$ to denote $E[Y|\sigma(X)]$). 

    Let's first guess and then verify. Given $X=x$, we believe $Y\sim \text{Unif}[-\sqrt{1-x^2},\sqrt{1-x^2}]$. Thus,
    \begin{align*}
        E[Y^2|X=x] &=\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \frac{1}{2\sqrt{1-x^2}}y^2 dy\\
        &= \frac{1}{2\sqrt{1-x^2}}\left[\frac{1}{3}y^3\right]_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}\\
        &=\frac{1}{3}(1-x^2).
    \end{align*}
    So, our guess is $E[Y^2|\G]=\frac{1}{3}(1-X^2)$. Let's check the two properties. The random variable $Z=\frac{1}{3}(1-X^2)$ is a continuous function of $X$ and hence $\G$-measurable. The random variable $Z$ is also bounded and hence integrable. Lastly, if $B \in \G$, then $B=\{X\in A\}$ for some Borel set $A$. It follows that
    \begin{align*}
        E[ Y^2\one_B]&= E[Y^2\one_{X \in A}]\\
        &=\frac{1}{\pi}\int_{-1}^1 \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} y^2\one_{A}(x) dydx\\
        &=\frac{1}{\pi}\int_A \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}y^2dydx\\
        &=\int_A \frac{2\sqrt{1-x^2}}{\pi} \frac{1}{3}(1-x^2)dx\\
        &=E\left[\one_{X \in A}\frac{1}{3}(1-X^2)\right]\\
        &= E\left[\one_B\frac{1}{3}(1-X^2)\right]
    \end{align*}
    the last line holds because $\frac{2\sqrt{1-x^2}}{\pi}$ is the marginal density of $X$.
\end{example}
We will now prove Lemma \ref{uniqueness-lemma}.
\begin{proof}
    Suppose $Y_1,Y_2$ are $\G$ measurable and 
    \[E[\one_B Y_1]=E[\one_B Y_2] = E[\one_B X], \]
    for all $B \in \G$. Let $B = \{Y_1 > Y_2\}$. Since $Y_1,Y_2$ are both $\G$-measurable, $B \in \G$. Thus, by assumption, $E[\one_B(Y_1-Y_2)]=0$. Since $\one_B(Y_1-Y_2)$ is non-negative, this implies $P(\one_B(Y_1-Y_2)=0)=1$ and hence $P(Y_1 \le Y_2)=1$. By symmetry, we must have $P(Y_2 \le Y_1)=1$ and thus $P(Y_1=Y_2)=1$ and $Y_1=Y_2$, almost surely.
\end{proof}
If we return to example 2, we can see that $E[Y^2|X]$ is only defined up to a set of measure zero. Indeed, the random variable 
\[Z' = \begin{cases}
    \frac{1}{3}(1-X^2) & \text{if } X \neq 0, \\
    100 & \text{if } X =0.
\end{cases}\]
is also a candidate for $E[Y^2|X]$. Our next goal is to prove the following theorem.
\begin{theorem}\label{existence}
    For any probability space $(\Om,\F,P)$, any integrable random variable $X$ and any sub-$\sigma$-algebra $\G$, the conditional expectation $E[X|\G]$ exists and is unique almost surely.
\end{theorem}
We already proved uniqueness, and so we only need to prove existence. We'll start with a lemma where we allow ourselves an additional assumption.
\begin{lemma}
    If $X \in L^2(\Om,\F,P)$, then $E[X|\G]$ exists.
\end{lemma}
The notation $X \in L^2(\Om,\F,P)$ means that $X$ is a random variable with respect to $(\Om,\F,P)$ and $E[X^2] < \infty$.
\begin{proof}
    Assume $X \in L^2$ and let
    \[a = \inf_{Y \in L^2(\G)} E(X-Y)^2. \]
    Then there exist $Y_1,Y_2,\ldots \in L^2(\G)$ such that 
    \[\lim_{n \to \infty} E(X-Y_n)^2 = a. \]
    Recall the parallelogram identity, for all $u,v \in \R$,
    \[(u-v)^2+(u+v)^2 = 2u^2+2v^2. \]
    Now fix any $n,m$ and take $u = X-Y_n$ and $v = X-Y_m$ and so 
    \[(Y_m-Y_n)^2 + (2X-Y_n-Y_m)^2 = 2(X-Y_n)^2+2(X-Y_m)^2. \]
    Taking expectations and rearranging, we get
    \[E(Y_m-Y_n)^2 = 2E(X-Y_n)^2+2E(X-Y_m)^2 - 4E\left(X- \frac{Y_m+Y_n}{2}\right)^2. \]
    Now, $\frac{Y_m+Y_n}{2} \in L^2(\G)$ and so $E\left(X-\frac{Y_m+Y_n}{2}\right)^2 \ge a$ and thus
    \[E(Y_m-Y_n)^2 \le 2E(X-Y_n)^2+2E(X-Y_m)^2 - 4a. \]
    As $n,m \to \infty$, $E(X-Y_n)^2, E(X-Y_m)^2 \to a$ and so 
    \[\limsup_{n,m\to \infty} E(Y_m-Y_n)^2 \le 0, \]
    which implies $\lim\limits_{n,m \to \infty} \E(Y_m-Y_n)^2 = 0$. Thus, $\{Y_n\}_{n =1}^\infty$ is a Cauchy sequence in $L^2(\G)$. The space $L^2(\G)$ is a complete Hilbert space and so there exists $Y \in L^2(\G)$ such that $Y_n \to Y$. It follows that $E(X-Y)^2 = a$.

    We now wish to show that $Y = E[X|\G]$. Take any $B \in \G$. We wish to show $E[\one_B X] = E[\one_B Y]$. For any $\la \in \R$, $Y+\la \one_B \in L^2(\G)$ and so $E(X-Y-\la\one_B)^2 \ge a$ and furthermore
    \[E(X-Y-\la\one_B)^2 = E(X-Y)^2 -2\la E[(X-Y)\one_B] +\la^2P(B).\]
    Since the above quadratic is minimized when $\la = 0$, we must have $2E[(X-Y)\one_B]=0$ as required.
\end{proof} 
\begin{remark}
    It might be helpful to think of $E[X|\G]$ as regressing $X$ on $\G$. What we were doing above is a form of least squares.
\end{remark}
In the next class we will prove the result for integrable $X$. We will see that most of the work is done. This is because the above lemma implies that $E[X|\G]$ exists for simple $X$. The next step will be a familiar limiting argument.
\end{document}