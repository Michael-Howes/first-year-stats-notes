\include{preamble}
\include{definitions}



\title{STATS310A - Lecture 18}
\author{Persi Diaconis\\ Scribed by Michael Howes}
\date{11/30/21}

\pagestyle{fancy}
\fancyhf{}
\rhead{STATS310A - Lecture 18}
\lhead{11/30/21}
\rfoot{Page \thepage}

\begin{document}
\maketitle
\tableofcontents
\section{Announcements}
\begin{itemize}
    \item Thursday's lecture will also be on Zoom.
    \item Wednesday's office hours will be on Zoom.
\end{itemize}
Here goes.
\section{Helly's selection theorem}
\begin{thrm}[Helly's selection theorem]
    If $\{F_n\}_{n=1}^\infty$ are any cumulative distribution functions on $\R$, then there exists a subsequence $n_k$ and a monotone, right continuous function $F$ such that $F_{n_k}(x) \to F(x)$ for all $x$ such that $F$ is continuous at $x$.
\end{thrm}
Before we prove the above theorem it is important to note that $F$ need no be a cumulative distribution function.
\begin{proof}
    Let $\{r_i\}_{i=1}^\infty$ be an enumeration of $\Q$. We can form the array
    \begin{align*}
        \begin{matrix}
            F_1(r_1)&F_2(r_1)&F_3(r_1)&\ldots\\
            F_1(r_2)&F_2(r_2)&F_3(r_2)&\ldots\\
            F_1(r_3)&F_2(r_3)&F_3(r_3)&\ldots\\
            \vdots&\vdots&\vdots&\ddots
        \end{matrix}
    \end{align*}
    Each row is bounded since $F_n(x) \in [0,1]$ for all $n$ and $x \in \R$. Thus Cantor's diagonal argument implies that there exists a subsequence $n_k$ and a function $G : \Q \to \R$ such that $F_{n_k}(r) \to G(r)$ for all $r \in \Q$.

    Note that if $r<s$, then $F_{n_k}(r) \le F_{n_k}(s)$ for all $k$ and so $G(r)\le G(s)$. Now define
    \[F(x)=\inf\{G(r): r>x, r\in \Q\}.\]
    Since $G$ is non-decreasing, $F$ is also non-decreasing. We will now show that $F$ is right continuous. Given $x$ and $\eps > 0$, find $r > x$ such that $G(r) < F(x)+\eps$. If $x < y < r$, then 
    \[F(x) \le F(y) \le G(r) < F(x)+\eps.\]
    Thus, $F$ is right continuous. Now we just need to prove that if $x$ is a continuity point of $F$, then $F_{n_k}(x) \to F(x)$.

    This is elementaty but (slightly tedious). Given $x$ a continuity point and $\eps > 0$, choose $y < x$ such that $F(x)-\eps < F(y)$. Next choose rational numbers $r$ and $s$ so that $y < r< x< s$ and
    \[G(s) < F(x)+\eps. \]
    It follows that 
    \[F(x)-\eps < F(y)\le G(r) \le G(s) < F(x)+\eps. \]
    We also have $F_{n_k}(r) \le F_{n_k}(x)\le F_{n_k}(s)$ for all $k$ and so
    \begin{align*}
        F(x)-\eps&\le G(r)\\
         &=\lim_k F_{n_k}(r)\\
        &\le \underline{\lim}_k F_{n_k}(x)\\
        & \le \overline{\lim}_k F_{n_k}(x)\\
        & \le \lim_k F_{n_k}(s)\\
        &= G(s)\\
        &\le F(x)-\eps.
    \end{align*}
    Thus $\underline{\lim}F_{n_k}(x)$ and $\overline{\lim}F_{n_k}(x)$ are both within $\eps$ of $F(x)$. Since $\eps$ was arbitrary we can conclude that $\lim_k F_{n_k}(x)=F(x)$.
\end{proof}
\begin{ex}
    As mentioned before, the limiting function $F$ need not be a cumulative distribution function. For example,
    \begin{itemize}
        \item If $F_n$ is the cumulative distribution function of a point mass of $n$, then $F_n(x) \to 0$ for all $x$.
        \item If $F_n$ is the cumulative distribution function of a point mass of $-n$, then $F_n(x) \to 1$ for all $x$.
    \end{itemize}
\end{ex}
The kind of convergence in the statement of the Helly's selection theorem is called \emph{vague convergence}.
\section{Tightness}
How can we be sure that the limit function $F$ in Helly's selection theorem is a distribution? It turns out that the key property is tightness.
\begin{defn}
A family of probability distributions $\{\mu_n\}$ on $\R$ is \emph{tight} if for all $\eps > 0$, there exists $a < b$ such that $\mu_n([a,b]) >1-\eps$ for all $n$. 
\end{defn}
We will sometimes say $\{\mu_n\}$ are ``almost compactly supported'' to mean $\{\mu_n\}$ is tight.
\begin{thrm}
    Let $\{\mu_n\}$ be a family of probability distributions on $\R$. Then $\{\mu_n\}$ is tight if and only if for every subsequence $n_k$, there exists a further subsequence $n_{k_i}$ and a probability distribution $\mu$ such that $\mu_{n_{k_i}} \Rightarrow \mu$ as $i \to \infty$.
\end{thrm}
\begin{proof}
    We will also use that if $\{\mu_n\}$ is tight, then for every subsequence $n_k$, there exists a further subsequence $n_{k_i}$ and a probability distribution $\mu$ such that $\mu_{n_{k_i}} \Rightarrow \mu$ as $i \to \infty$. Thus we will only prove this direction.

    Let $\{\mu_n\}$ be a tight family of probability distributions with corresponding cumulative distribution functions $F_{n_k}$. Let $n_k$ be a subsequence. By Helly's selection theorem, there exists a further subsequence $n_{k_i}$ and a monotone right-continuous function $F$ such that $F_{n_{k_i}}(x) \to F(x)$ for all $x$ such that $F$ is continuous at $x$. 
    
    We wish to shown that $F$ is a cumulative distribution function for some probability measure $\mu$ as this will imply that $\mu_n \Rightarrow \mu$. To show that $F$ is a cumulative distribution function, it suffices to show that $\lim\limits_{x \to \infty}F(x)=1$ and $\lim\limits_{x \to - \infty}F(x)=0$. We know that $F(x) \in [0,1]$ for all $x$ since each $F_{n_{k_i}}$ is a cumulative distribution function. Furthermore since $\{\mu_n\}$ is tight, for every $\eps>0$ there exist $a<b$ such that $F$ is continuous at $a$ and $b$ and for all $i$
    \[F_{n_{k_i}}(b)-F_{n_{k_i}}(a) > 1-\eps.\]
    By  taking a limit we have $F(b)-F(a) \ge 1-\eps$ which is sufficient to conclude that $F$ has the correct limits.
\end{proof}
\begin{remark}
    If $\int_\R \abs{x}\mu_n(dx)$ is uniformly bounded in $n$, then the family $\{\mu_n\}$ is tight.
    
    Likewise, if $\int_\R f(\abs{x})\mu_n(dx)$ is uniformly bounded in $n$ for some unbounded monotone function $f:\R_+ \to \R_+$, then $\{\mu_n\}$ is tight. Both of these claims follow by Markov's inequality for monotonically increasing functions.
\end{remark}
\begin{remark}
    All of what we have done works for a complete seperable metric space $\X$. We have to work with $\B(\X)$ the Borel $\sigma$-algebra on $\X$ which is the $\sigma$-algebra generated by the open subsets of $\X$. A sequence of probabilities $\mu_n$ on $(\X,\B(\X))$ converges weak* to $\mu$ if for all bounded and continuous functions $f$ on $\X$, we have 
    \[\int_\X f(x)\mu_n(dx)\to \int_\X f(x)\mu(dx). \]
    In this setting, we say that $\{\mu_n\}$ is tight is for all $\eps > 0$, there exists a compact set $K \subseteq \X$ so that 
    \[\mu_n(K) > 1-\eps,\]
    for all $n$. Some references for this topic are:
    \begin{itemize}
        \item Billingsley ``Convergence of probability measures.''
        \item Kallenberg ``Probability Theory'' ($3^{rd}$ edition).
        \item Dudley ``Real Analysis and Probability.''
    \end{itemize}
    All three are great books.
\end{remark}
\section{The continuity theorem}
The below theorem states that pointwise convergence of characteristic functions is exactly convergence in distribution. This was a missing link in Laplace's argument for the central limit theorem.
\begin{thrm}
    Let $\{F_n\},F$ be cumulative distribution functions with characteristic functions $\phi_n,\phi$, then $F_n \Rightarrow F$ if and only if for all $t \in \R$, $\phi_n(t)\to\phi(t)$.
\end{thrm}
\begin{proof}
    Let $\mu_n$ and $\mu$ be the probability distributions corresponding to $F_n$ and $F$. The functions $x\mapsto \cos(tx)$ and $x \mapsto \sin(tx)$ are both bounded and continuous, then if $F_n \Rightarrow F$, then 
    \[\phi_n(t) = \int_\R (\cos(tx)+i\sin(tx))\mu_n(dx) \to \int_\R (\cos(tx)+i\sin(tx))\mu(dx) = \phi(t). \]
    Now suppose that $\phi_n(t)\to\phi(t)$ for all $t$. We will show later that this implies that $\{\mu_n\}$ is tight. Now suppose that $F_n \not\Rightarrow F$. Then there exists some $x \in \R$ such that $F$ is continuous at $x$ but $F_n(x) \not\to F(x)$. Thus there exists a subsequence $n_k$ and $\eps > 0$ such that $\abs{F_{n_k}(x)-F_(x)} > \eps$ for all $k$. Since we will show that $\phi_n$ is tight, this implies that there exists a cumulative distribution function $G$ and a further subsequence $n_{k_i}$ such that $F_{n_{k_i}}\Rightarrow G$. Note that we cannot have $G = F$ as this will imply that $G$ is continuous at $x$ and hence $F_{n_{k_i}}(x) \to G(x)=F(x)$. 

    Let $\phi_G$ be the characteristic function of $G$. Since $F_{n_{k_i}} \Rightarrow G$, we have $\phi_{n_{k_i}}(t)\to \phi_G(t)$ and thus $\phi_G(t)=\phi(t)$. By the uniquness theorem (which we state below and will prove next lecture) this implies that $G=F$, a contradiction.

    It thus remains to show that $\{\mu_n\}$ is tight. For $u >0$, consider the quantity
    \[\frac{1}{u}\int_{-u}^u (1-\phi(t))dt. \]
    By Fubinni's theorem we have
    \begin{align*}
        \frac{1}{u}\int_{-u}^u 1 -\phi(t)dt &= \int_{-\infty}^\infty \frac{1}{u}\int_{-u}^u 1-e^{itx}dt \mu(dx)\\
        &= 2 \int_{-\infty}^\infty \left[1-\frac{\sin(ux)}{ux}\right]\mu(dx)\\
        &\ge 2\int_{\{x:\abs{x}>2/u\}} 1-\frac{\sin(ux)}{ux}\mu(dx)\\
        &\ge 2\int_{\{x:\abs{x}>2/u\}} 1-\frac{1}{ux}\mu(dx)\\
        &\ge 2\int_{\{x:\abs{x}>2/u\}} \frac{1}{2}\mu(dx)\\
        &=\mu\left(\left\{x:\abs{x} > \frac{2}{u}\right\}\right)
    \end{align*}
    Now $\phi(t)$ is continuous and $\phi(0)=1$. Thus for all $t>0$, there exists $u$ sufficiently small such that $\abs{1-\phi(t)}< \frac{\eps}{2}$, given $\abs{t} <\eps$. This implies that 
    \[\abs{\frac{1}{u}\int_{-u}^u 1-\phi(t)dt} < \eps. \]
    We know that $\phi_n(t) \to \phi(t)$ for all $t \in \R$. Thus by the bouunded convergence theorem, there exists $n_0$ such that if $n \ge n_0$, then 
    \[\mu_n\left(\{x:\abs{x}> \frac{2}{u}\}\right) \le \frac{1}{u}\int_{-u}^u 1-\phi_n(t)dt \le 2 \eps. \]
    By taking $u$ smaller we can ensure that $u>0$ and 
    \[\mu_n\left(\{x:\abs{x}> \frac{2}{u}\}\right) \le 2\eps, \]
    for $n=1,2,\ldots,n_0-1$. Thus we have shown that $\{\mu_n\}$ is tight. 
\end{proof}
If you'd like to learn more about Laplace and his proof of the central limit theorem, search for ``Steve Stigler, Laplace.''
\begin{remark}
    Two comments.
    \begin{itemize}
        \item The continuity theorem is a substantial theorem that uses topology and Helly's selection theorem.
        \item Our proof relies on the uniqueness theorem which remains to be proven.
    \end{itemize}
\end{remark}
\end{document}