\include{preamble}
\include{definitions}



\title{STATS310A - Lecture 18}
\author{Persi Diaconis\\ Scribed by Michael Howes}
\date{11/30/21}

\pagestyle{fancy}
\fancyhf{}
\rhead{STATS310A - Lecture 18}
\lhead{11/30/21}
\rfoot{Page \thepage}

\begin{document}
\maketitle
\tableofcontents
\section{Announcements}
\begin{itemize}
    \item Thursday's lecture will also be on Zoom.
    \item Wednesday's office hours will be on Zoom.
\end{itemize}
Here goes.
\section{Helly's selection theorem}
\begin{thrm}[Helly's selection theorem]
    If $\{F_n\}_{n=1}^\infty$ are any cumulative distribution functions on $\R$, then there exists a subsequence $n_k$ and a monotone, right continuous function $F$ such that $F_{n_k}(x) \to F(x)$ for all $x$ such that $F$ is continuous at $x$.
\end{thrm}
Before we prove the above theorem it is important to note that $F$ might not be a cumulative distribution function.
\begin{proof}
    Let $\{r_i\}_{i=1}^\infty$ be an enumeration of $\Q$. We can form the array
    \begin{align*}
        \begin{matrix}
            F_1(r_1)&F_2(r_1)&F_3(r_1)&\ldots\\
            F_1(r_2)&F_2(r_2)&F_3(r_2)&\ldots\\
            F_1(r_3)&F_2(r_3)&F_3(r_3)&\ldots\\
            \vdots&\vdots&\vdots&\ddots
        \end{matrix}
    \end{align*}
    Each row is bounded since $F_n(x) \in [0,1]$ for all $n$ and $x \in \R$. Thus Cantor's diagonal argument implies that there exists a subsequence $n_k$ and a function $G : \Q \to \R$ such that $F_{n_k}(r) \to G(r)$ for all $r \in \Q$.

    Note that if $r<s$, then $F_{n_k}(r) \le F_{n_k}(s)$ for all $k$ and so $G(r)\le G(s)$. Now define
    \[F(x)=\inf\{G(r): r>x, r\in \Q\}.\]
    Since $G$ is non-decreasing, $F$ is also non-decreasing. We will now show that $F$ is right continuous. Given $x$ and $\eps > 0$, find $r > x$ such that $G(r) < F(x)+\eps$. If $x < y < r$, then 
    \[F(x) \le F(y) \le G(r) < F(x)+\eps.\]
    Thus, $F$ is right continuous. Now we just need to prove that if $x$ is a continuity point of $F$, then $F_{n_k}(x) \to F(x)$.

    This is elementaty but (slightly) tedious. Given $x$ a continuity point and $\eps > 0$, choose $y < x$ such that $F(x)-\eps < F(y)$. Next choose rational numbers $r$ and $s$ so that $y < r< x< s$ and
    \[G(s) < F(x)+\eps. \]
    It follows that 
    \[F(x)-\eps < F(y)\le G(r) \le G(s) < F(x)+\eps. \]
    We also have $F_{n_k}(r) \le F_{n_k}(x)\le F_{n_k}(s)$ for all $k$ and so
    \begin{align*}
        F(x)-\eps&\le G(r)\\
         &=\lim_k F_{n_k}(r)\\
        &\le \underline{\lim}_k F_{n_k}(x)\\
        & \le \overline{\lim}_k F_{n_k}(x)\\
        & \le \lim_k F_{n_k}(s)\\
        &= G(s)\\
        &\le F(x)-\eps.
    \end{align*}
    Thus $\underline{\lim}F_{n_k}(x)$ and $\overline{\lim}F_{n_k}(x)$ are both within $\eps$ of $F(x)$. Since $\eps$ was arbitrary we can conclude that $\lim_k F_{n_k}(x)=F(x)$.
\end{proof}
\begin{ex}
    As mentioned before, the limiting function $F$ need not be a cumulative distribution function. For example,
    \begin{itemize}
        \item If $F_n$ is the cumulative distribution function of a point mass at $n$, then $F_n(x) \to 0$ for all $x$.
        \item If $F_n$ is the cumulative distribution function of a point mass at $-n$, then $F_n(x) \to 1$ for all $x$.
    \end{itemize}
    Neither of these limits are cumulative distribution functions.
\end{ex}
The kind of convergence in the statement of the Helly's selection theorem is called \emph{vague convergence}.
\section{Tightness}
How can we be sure that the limit function $F$ in Helly's selection theorem is a distribution? It turns out that the key property is tightness.
\begin{defn}
A family of probability distributions $\{\mu_n\}$ on $\R$ is \emph{tight} if for all $\eps > 0$, there exists $a < b$ such that $\mu_n([a,b]) >1-\eps$ for all $n$. 
\end{defn}
We will sometimes say $\{\mu_n\}$ are ``almost compactly supported'' to mean $\{\mu_n\}$ is tight.
\begin{thrm}
    Let $\{\mu_n\}$ be a family of probability distributions on $\R$. Then $\{\mu_n\}$ is tight if and only if for every subsequence $n_k$, there exists a further subsequence $n_{k_i}$ and a probability distribution $\mu$ such that $\mu_{n_{k_i}} \Rightarrow \mu$ as $i \to \infty$.
\end{thrm}

In the remaining lectures, we will only use that if $\{\mu_n\}$ is tight, then for every subsequence $n_k$, there exists a further subsequence $n_{k_i}$ and a probability distribution $\mu$ such that $\mu_{n_{k_i}} \Rightarrow \mu$ as $i \to \infty$. Thus we will only prove this direction of the above theorem.
\begin{proof}
    Let $\{\mu_n\}$ be a tight family of probability distributions with corresponding cumulative distribution functions $F_{n}$. Let $n_k$ be a subsequence. By Helly's selection theorem, there exists a further subsequence $n_{k_i}$ and a monotone right-continuous function $F$ such that $F_{n_{k_i}}(x) \to F(x)$ for all $x$ such that $F$ is continuous at $x$. 
    
    We wish to shown that $F$ is a cumulative distribution function for some probability measure $\mu$ as this will imply that $\mu_n \Rightarrow \mu$. To show that $F$ is a cumulative distribution function, it suffices to show that $\lim\limits_{x \to \infty}F(x)=1$ and $\lim\limits_{x \to - \infty}F(x)=0$. We know that $F(x) \in [0,1]$ for all $x$ since each $F_{n_{k_i}}$ is a cumulative distribution function. Furthermore since $\{\mu_n\}$ is tight, for every $\eps>0$ there exist $a<b$ such that $F$ is continuous at $a$ and $b$ and for all $i$
    \[F_{n_{k_i}}(b)-F_{n_{k_i}}(a) > 1-\eps.\]
    By  taking a limit we have $F(b)-F(a) \ge 1-\eps$ which is sufficient to conclude that $F$ has the correct limits.
\end{proof}
\begin{remark}
    If $\int_\R \abs{x}\mu_n(dx)$ is uniformly bounded in $n$, then the family $\{\mu_n\}$ is tight.
    
    Likewise, if $\int_\R f(\abs{x})\mu_n(dx)$ is uniformly bounded in $n$ for some unbounded monotone function $f:\R_+ \to \R_+$, then $\{\mu_n\}$ is tight. Both of these claims follow by Markov's inequality for monotonically increasing functions.
\end{remark}
\begin{remark}
    All of what we have done works for a complete seperable metric space $\X$. We have to work with $\B(\X)$ the Borel $\sigma$-algebra on $\X$ which is the $\sigma$-algebra generated by the open subsets of $\X$. A sequence of probabilities $\mu_n$ on $(\X,\B(\X))$ converges weak* to $\mu$ if for all bounded and continuous functions $f$ on $\X$, we have 
    \[\int_\X f(x)\mu_n(dx)\to \int_\X f(x)\mu(dx). \]
    In this setting, we say that $\{\mu_n\}$ is tight is for all $\eps > 0$, there exists a compact set $K \subseteq \X$ so that 
    \[\mu_n(K) > 1-\eps,\]
    for all $n$. Some references for this topic are:
    \begin{itemize}
        \item Billingsley ``Convergence of probability measures.''
        \item Kallenberg ``Probability Theory'' ($3^{rd}$ edition).
        \item Dudley ``Real Analysis and Probability.''
    \end{itemize}
    All three are great books.
\end{remark}
\section{The continuity theorem}
The below theorem states that pointwise convergence of characteristic functions is exactly convergence in distribution. This was a missing link in Laplace's argument for the central limit theorem.
\begin{thrm}
    Let $\{F_n\},F$ be cumulative distribution functions with characteristic functions $\phi_n,\phi$, then $F_n \Rightarrow F$ if and only if for all $t \in \R$, $\phi_n(t)\to\phi(t)$.
\end{thrm}
\begin{proof}
    Let $\mu_n$ and $\mu$ be the probability distributions corresponding to $F_n$ and $F$. The functions $x\mapsto \cos(tx)$ and $x \mapsto \sin(tx)$ are both bounded and continuous. Thus if $F_n \Rightarrow F$, then 
    \[\phi_n(t) = \int_\R (\cos(tx)+i\sin(tx))\mu_n(dx) \to \int_\R (\cos(tx)+i\sin(tx))\mu(dx) = \phi(t). \]
    Now suppose that $\phi_n(t)\to\phi(t)$ for all $t$. We will show later that this implies that $\{\mu_n\}$ is tight. Now suppose that $F_n \not\Rightarrow F$. Then there exists some $x \in \R$ such that $F$ is continuous at $x$ but $F_n(x) \not\to F(x)$. Thus there exists a subsequence $n_k$ and $\eps > 0$ such that $\abs{F_{n_k}(x)-F_(x)} > \eps$ for all $k$. Since we will show that $\phi_n$ is tight, this implies that there exists a cumulative distribution function $G$ and a further subsequence $n_{k_i}$ such that $F_{n_{k_i}}\Rightarrow G$. Note that we cannot have $G = F$ as this will imply that $G$ is continuous at $x$ and hence $F_{n_{k_i}}(x) \to G(x)=F(x)$. 

    Let $\phi_G$ be the characteristic function of $G$. Since $F_{n_{k_i}} \Rightarrow G$, we have $\phi_{n_{k_i}}(t)\to \phi_G(t)$ and thus $\phi_G(t)=\phi(t)$. By the uniquness theorem (which we state below and will prove next lecture) this implies that $G=F$, a contradiction.

    It thus remains to show that $\{\mu_n\}$ is tight. For $u >0$, consider the quantity
    \[\frac{1}{u}\int_{-u}^u (1-\phi(t))dt. \]
    By Fubinni's theorem we have
    \begin{align*}
        \frac{1}{u}\int_{-u}^u 1 -\phi(t)dt &= \int_{-\infty}^\infty \frac{1}{u}\int_{-u}^u 1-e^{itx}dt \mu(dx)\\
        &= 2 \int_{-\infty}^\infty \left[1-\frac{\sin(ux)}{ux}\right]\mu(dx)\\
        &\ge 2\int_{\{x:\abs{x}>2/u\}} 1-\frac{\sin(ux)}{ux}\mu(dx)\\
        &\ge 2\int_{\{x:\abs{x}>2/u\}} 1-\frac{1}{ux}\mu(dx)\\
        &\ge 2\int_{\{x:\abs{x}>2/u\}} \frac{1}{2}\mu(dx)\\
        &=\mu\left(\left\{x:\abs{x} > \frac{2}{u}\right\}\right)
    \end{align*}
    Now $\phi(t)$ is continuous and $\phi(0)=1$. Thus for all $t>0$, there exists $u>0$ sufficiently small such that $\abs{1-\phi(t)}< \frac{\eps}{2}$, given $\abs{t} <u$. This implies that 
    \[\abs{\frac{1}{u}\int_{-u}^u 1-\phi(t)dt} < \eps. \]
    We know that $\phi_n(t) \to \phi(t)$ for all $t \in \R$. Thus by the bouunded convergence theorem, there exists $n_0$ such that if $n \ge n_0$, then 
    \[\mu_n\left(\left\{x:\abs{x}> \frac{2}{u}\right\}\right) \le \frac{1}{u}\int_{-u}^u 1-\phi_n(t)dt \le 2 \eps. \]
    By taking $u$ smaller we can ensure that $u>0$ and 
    \[\mu_n\left(\left\{x:\abs{x}> \frac{2}{u}\right\}\right) \le 2\eps, \]
    for $n=1,2,\ldots,n_0-1$. Thus we have shown that $\{\mu_n\}$ is tight. 
\end{proof}
If you'd like to learn more about Laplace and his proof of the central limit theorem, search for ``Steve Stigler, Laplace.''
\begin{remark}
    Two comments.
    \begin{itemize}
        \item The continuity theorem is a substantial theorem that uses topology and Helly's selection theorem.
        \item Our proof relies on the uniqueness theorem which we have not yet proved.
    \end{itemize}
\end{remark}

\section{Uniqueness theorem}
\begin{thrm}[Inversion theorem]
    Let $\mu$ be a probability on $\R$ with characteristic function $\phi$. If $a<b$, then,
    \[\mu((a,b))+\frac{1}{2}\mu(\{a,b\}) = \lim_{T \to \infty}\frac{1}{2\pi} \int_{-T}^T \frac{e^{-ita}-e^{-itb}}{it}\phi(t)dt. \]
\end{thrm}
The uniqueness theorem is a corollary.
\begin{cor}
    If $\mu$ and $\nu$ are probability measures and $\phi_\mu = \phi_\nu$, then $\mu=\nu$.
\end{cor}
\begin{proof}
    The family of sets 
    \[\Po = \{(a,b): \mu(\{a\})=\mu(\{b\})=\nu(\{a\})=\nu(\{b\})=0\},\] 
    is a $\pi$-system that generates the Borel $\sigma$-algebra. Since $\phi_\mu = \phi_\nu$, the inversion theorem implies that $\mu$ and $\nu$ agree on $\Po$. By the trusty old $\pi-\la$ theorem, this implies that $\mu=\nu$. 
\end{proof}
The inversion theorem does not give a formula for $\mu$ in terms of $\phi$ since it involves a limit. If we put additional assumptions on $\phi$, then we do get a formula for $\mu$. 
\begin{thrm}
    If $\int_{-\infty}^\infty \abs{\phi(t)}dt < \infty$, then $\mu$ has a bounded density $f$ and 
    \[f(y) = \frac{1}{2\pi}\int_{-\infty}^\infty e^{-ity}\phi(t)dt. \]
\end{thrm}
We will prove these theorems on Thursday.
\section{Generalizations}
There are versions of characteristic functions for measures on lots of spaces other than $\R$. For example one can work with measures on groups and do Fourier analysis on non-commutative groups. See ``Group Representations in Probability and Statistics'' by Persi. We won't talk about characteristic functions on non-commutative groups here but we will talk about characteristic functions on $\R^d$.
\begin{defn}
    Let $\mu$ be a probability distribution on $\R^d$. The \emph{characteristic function of $\mu$} is the function $\phi_\mu : \R^d \to \C$ given by 
    \[\phi_u(t) = \E_\mu[e^{it\cdot x}], \]
    where $t\cdot x$ denotes the dot product between $t$ and $x$.
\end{defn}
All of the theorems we studied on $\R$, hold for characteristic functions on $\R^d$. We also have the following result.
\begin{prop}[Cramer-Wold device]
    If $X \in \R^d$ is a random vector and we know the distribution of $\sum_{j=1}^d a_j X_j$ for all $a \in \R^d$,  then we know the distribution of $X$.
\end{prop}
\begin{proof}
    If we know the distribution of $t \cdot X$ for all $t \in \R^d$, then we know $\phi(t) = \E[e^{it\cdot  X}]$ for all $t$. Thus we know the characteristic function of $X$ and hence the distribution of $X$.
\end{proof}
\begin{ex}
    The Cramer-Wold device can be used to a prove multivariate central limit theorems for $(X_1^{(n)},\ldots,X_d^{(n)})$ from univariate central limit theorems for $t\cdot X^{(n)}$. For example, if $X_1,X_2,\ldots$ are i.i.d. in $\R^d$ with mean $\mu$ and covariance matrix $\Sigma$. If $S_n = \frac{1}{n}\sum_{j=1}^n(X_j-\mu)$, then 
    \[\sqrt{n}S_n \Rightarrow \Na_d(0,\Sigma), \]
    by the univariate central limit theorem and the Cramer-Wold device. There are also multivariate central limit theorems for triangular arrays.
\end{ex}
\begin{remark}
    Some final comments:
    \begin{itemize}
        \item \underline{Please} look in the book to see how similar the proof of the central limit theorem is to the one that we proved in class. They both use a swapping argument.
        \item Persi would have liked to have talked about infinitely divisible laws but there wasn't time this quarter.
        \item There are all kinds of tricks, maneuvers and proofs that are acheivable with charactertistic functions and the full power of complex analysis. The best source for this is W. Feller - ``An introduction to probability and it's applications'' Vol II (2nd edition) chapter 15. For example, these tools can prove the following:
        \begin{thrm}
            Suppose $X,Y$ are independent and there exist non-zero $a,b,c,d\in \R$, such that $(aX+bY, cX+dY)$ has the same distribution as $(X,Y)$, then $X$ and $Y$ are normal.
        \end{thrm}
    \end{itemize}
    
\end{remark}


\end{document}